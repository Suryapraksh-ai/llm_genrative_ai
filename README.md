# llm_genrative_ai
# ğŸ¨ CLIP + VQGAN Text-to-Image Generator & Video Interpolation

This project uses **OpenAI's CLIP** and **VQGAN** (via Taming Transformers) to generate **images from text prompts**, and then **interpolates** between generated images to create a **video animation**.

> âœ… Fully implemented in **Google Colab**  
> âœ… Combines vision-language and generative modeling  
> âœ… Supports prompt engineering and exclusion terms  
> âœ… Outputs **MP4 video** from your prompt-driven imagination!

---

## ğŸŒŸ Demo Output

| ğŸ¬ Prompt-based Interpolation Video |
|------------------------------------|
| A blue tree â†’ Kids playing in moon â†’ Dancing flowers |

> **Watch generated video at the end of this notebook or GitHub Pages (if hosted)**

---

## ğŸ› ï¸ Tech Stack

- ğŸ§  **CLIP** (ViT-B/32) - OpenAI's vision-language model
- ğŸ§± **VQGAN** - Taming Transformers for image generation
- ğŸ” **Latent Interpolation** between vectors
- ğŸï¸ **Imageio** - for video creation
- ğŸ§ª PyTorch, Torchvision, NumPy, Matplotlib

---

## ğŸ“¦ Installation & Setup (Google Colab Recommended)

```bash
# Clone Required Repositories
git clone https://github.com/openai/CLIP.git
git clone https://github.com/CompVis/taming-transformers

# Install Python Dependencies
pip install --no-deps ftfy regex tqdm
pip install omegaconf==2.0.0 pytorch-lightning==1.0.8
pip install einops taming-transformers-rom1504
pip install numpy==1.21.6
