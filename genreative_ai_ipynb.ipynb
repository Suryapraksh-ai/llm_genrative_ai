{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4QcnZh3pdxC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "WcoU8pDGpnjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLIP ARCHITECTURE\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "\n"
      ],
      "metadata": {
        "id": "QeUisKb4qNlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TAMING-TRANSFORMER ARCHITECTURE\n",
        "\n",
        "!git clone https://github.com/CompVis/taming-transformers"
      ],
      "metadata": {
        "id": "A_n47Arzq4UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We Need to install some more libraries as well\n",
        "\n",
        "!pip install --no-deps ftfy regex tqdm\n",
        "!pip install omegaconf==2.0.0 pytorch-lightning==1.0.8\n",
        "!pip uninstall torchtext --yes\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "q-3FfIoXrWex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import IMAGE, NUMPY,PANDAS,MATPLTOLIB libraries\n",
        "import PIL\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "e8pRZ3dQrcL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import PYTORCH libraries\n",
        "import torch, os, imageio, pdb, math\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF"
      ],
      "metadata": {
        "id": "xcYZVLk1sO5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install omegaconf\n"
      ],
      "metadata": {
        "id": "Iu7Su_5Xs80I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from CLIP import clip\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "4xLK8pzcsYeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## helper functions\n",
        "\n",
        "def show_from_tensor(tensor):\n",
        "  img = tensor.clone()\n",
        "  img = img.mul(255).byte()\n",
        "  img = img.cpu().numpy().transpose((1,2,0))\n",
        "\n",
        "  plt.figure(figsize=(10,7))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "def norm_data(data):\n",
        "  return (data.clip(-1,1)+1)/2 ### range between 0 and 1 in the result\n",
        "\n",
        "### Parameters\n",
        "learning_rate = .5\n",
        "batch_size = 1\n",
        "wd = .1\n",
        "noise_factor = .22\n",
        "\n",
        "total_iter=400\n",
        "im_shape = [450, 450, 3] # height, width, channel\n",
        "size1, size2, channels = im_shape\n"
      ],
      "metadata": {
        "id": "1zepaQkysfkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "bIcm-REdvFSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### CLIP MODEL ###\n",
        "clipmodel, _ = clip.load('ViT-B/32', jit=False)\n",
        "clipmodel.eval()\n",
        "print(clip.available_models())\n",
        "\n",
        "print(\"Clip model visual input resolution: \", clipmodel.visual.input_resolution)\n",
        "\n",
        "device=torch.device(\"cuda:0\")\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "s6F67n29uob4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n"
      ],
      "metadata": {
        "id": "B_INlCxtvc3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Taming transformer instantiation\n",
        "\n",
        "%cd taming-transformers/\n",
        "\n",
        "!mkdir -p models/vqgan_imagenet_f16_16384/checkpoints\n",
        "!mkdir -p models/vqgan_imagenet_f16_16384/configs\n",
        "\n",
        "if len(os.listdir('models/vqgan_imagenet_f16_16384/checkpoints/')) == 0:\n",
        "   !wget 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1' -O 'models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt'\n",
        "   !wget 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1' -O 'models/vqgan_imagenet_f16_16384/configs/model.yaml'\n"
      ],
      "metadata": {
        "id": "JzKMuuxourad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install taming-transformers-rom1504\n"
      ],
      "metadata": {
        "id": "uMaxFGzPwTS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall numpy\n"
      ],
      "metadata": {
        "id": "WZsTaElKwox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy==1.21.6\n"
      ],
      "metadata": {
        "id": "brULvWipwyTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.Inf = np.inf  # Reassign np.Inf to np.inf"
      ],
      "metadata": {
        "id": "MG0gjjYsxVVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade numpy\n"
      ],
      "metadata": {
        "id": "RZwkQyDzxvLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cQG48KeZvRX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from taming.models.vqgan import VQModel\n",
        "\n",
        "# Add the necessary globals to handle custom objects\n",
        "import torch.serialization\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "torch.serialization.add_safe_globals([ModelCheckpoint])\n",
        "\n",
        "def load_config(config_path, display=False):\n",
        "   config_data = OmegaConf.load(config_path)\n",
        "   if display:\n",
        "     print(yaml.dump(OmegaConf.to_container(config_data)))\n",
        "   return config_data\n",
        "\n",
        "def load_vqgan(config, chk_path=None):\n",
        "  model = VQModel(**config.model.params)\n",
        "  if chk_path is not None:\n",
        "    state_dict = torch.load(chk_path, map_location=\"cpu\", weights_only=False)[\"state_dict\"]\n",
        "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "  return model.eval()\n",
        "\n",
        "def generator(x):\n",
        "  x = taming_model.post_quant_conv(x)\n",
        "  x = taming_model.decoder(x)\n",
        "  return x\n",
        "\n",
        "# Load config\n",
        "taming_config = load_config(\"./models/vqgan_imagenet_f16_16384/configs/model.yaml\", display=True)\n",
        "\n",
        "# Load VQGAN model\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "taming_model = load_vqgan(taming_config, chk_path=\"./models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt\").to(device)\n"
      ],
      "metadata": {
        "id": "CJ7osF6GwF1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Declare the values that we are going to optimize\n",
        "\n",
        "class Parameters(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Parameters, self).__init__()\n",
        "    self.data = .5*torch.randn(batch_size, 256, size1//16, size2//16).cuda() # 1x256x14x15 (225/16, 400/16)\n",
        "    self.data = torch.nn.Parameter(torch.sin(self.data))\n",
        "\n",
        "  def forward(self):\n",
        "    return self.data\n",
        "\n",
        "def init_params():\n",
        "  params=Parameters().cuda()\n",
        "  optimizer = torch.optim.AdamW([{'params':[params.data], 'lr': learning_rate}], weight_decay=wd)\n",
        "  return params, optimizer\n"
      ],
      "metadata": {
        "id": "Cj6Xj0YFywQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Encoding prompts and a few more things\n",
        "normalize = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "def encodeText(text):\n",
        "  t=clip.tokenize(text).cuda()\n",
        "  t=clipmodel.encode_text(t).detach().clone()\n",
        "  return t\n",
        "\n",
        "def createEncodings(include, exclude, extras):\n",
        "  include_enc=[]\n",
        "  for text in include:\n",
        "    include_enc.append(encodeText(text))\n",
        "  exclude_enc=encodeText(exclude) if exclude != '' else 0\n",
        "  extras_enc=encodeText(extras) if extras !='' else 0\n",
        "\n",
        "  return include_enc, exclude_enc, extras_enc\n",
        "\n",
        "augTransform = torch.nn.Sequential(\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomAffine(30, (.2, .2), fill=0)\n",
        ").cuda()\n",
        "\n",
        "Params, optimizer = init_params()\n",
        "\n",
        "with torch.no_grad():\n",
        "  print(Params().shape)\n",
        "  img= norm_data(generator(Params()).cpu()) # 1 x 3 x 224 x 400 [225 x 400]\n",
        "  print(\"img dimensions: \",img.shape)\n",
        "  show_from_tensor(img[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "5rWGcUGyy8DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load the CLIP model\n",
        "clipmodel, _ = clip.load(\"ViT-B/32\", device)\n",
        "\n",
        "# Normalization parameters\n",
        "normalize = torchvision.transforms.Normalize(\n",
        "    (0.48145466, 0.4578275, 0.40821073),\n",
        "    (0.26862954, 0.26130258, 0.27577711)\n",
        ")\n",
        "\n",
        "# Function to encode text\n",
        "def encodeText(text):\n",
        "    t = clip.tokenize(text).cuda()  # Tokenize and move to GPU\n",
        "    t = clipmodel.encode_text(t).detach().clone()  # Encode text and detach\n",
        "    return t\n",
        "\n",
        "# Function to create encodings for multiple prompts\n",
        "def createEncodings(include, exclude, extras):\n",
        "    include_enc = [encodeText(text) for text in include]\n",
        "    exclude_enc = encodeText(exclude) if exclude != '' else 0\n",
        "    extras_enc = encodeText(extras) if extras != '' else 0\n",
        "\n",
        "    return include_enc, exclude_enc, extras_enc\n",
        "\n",
        "# Augmentations for image transformation\n",
        "augTransform = torch.nn.Sequential(\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomAffine(30, (.2, .2), fill=0)\n",
        ").cuda()\n",
        "\n",
        "# Assuming the functions `init_params`, `generator`, `norm_data`, and `show_from_tensor` are defined\n",
        "Params, optimizer = init_params()\n",
        "\n",
        "with torch.no_grad():\n",
        "    print(Params().shape)\n",
        "    img = norm_data(generator(Params()).cpu())  # Ensure image is normalized\n",
        "    print(\"img dimensions: \", img.shape)\n",
        "    show_from_tensor(img[0])  # Assuming `show_from_tensor` is defined to display the image\n"
      ],
      "metadata": {
        "id": "ltlTro8-zFQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### create crops\n",
        "\n",
        "def create_crops(img, num_crops=32):\n",
        "  p=size1//2\n",
        "  img = torch.nn.functional.pad(img, (p,p,p,p), mode='constant', value=0) # 1 x 3 x 448 x 624 (adding 112*2 on all sides to 224x400)\n",
        "\n",
        "  img = augTransform(img) #RandomHorizontalFlip and RandomAffine\n",
        "\n",
        "  crop_set = []\n",
        "  for ch in range(num_crops):\n",
        "    gap1= int(torch.normal(1.2, .3, ()).clip(.43, 1.9) * size1)\n",
        "    offsetx = torch.randint(0, int(size1*2-gap1),())\n",
        "    offsety = torch.randint(0, int(size1*2-gap1),())\n",
        "\n",
        "    crop=img[:,:,offsetx:offsetx+gap1, offsety:offsety+gap1]\n",
        "\n",
        "    crop = torch.nn.functional.interpolate(crop,(224,224), mode='bilinear', align_corners=True)\n",
        "    crop_set.append(crop)\n",
        "\n",
        "  img_crops=torch.cat(crop_set,0) ## 30 x 3 x 224 x 224\n",
        "\n",
        "  randnormal = torch.randn_like(img_crops, requires_grad=False)\n",
        "  num_rands=0\n",
        "  randstotal=torch.rand((img_crops.shape[0],1,1,1)).cuda() #32\n",
        "\n",
        "  for ns in range(num_rands):\n",
        "    randstotal*=torch.rand((img_crops.shape[0],1,1,1)).cuda()\n",
        "\n",
        "  img_crops = img_crops + noise_factor*randstotal*randnormal\n",
        "\n",
        "  return img_crops\n"
      ],
      "metadata": {
        "id": "TtKkBDHpzyO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Show current state of generation\n",
        "\n",
        "def showme(Params, show_crop):\n",
        "  with torch.no_grad():\n",
        "    generated = generator(Params())\n",
        "\n",
        "    if (show_crop):\n",
        "      print(\"Augmented cropped example\")\n",
        "      aug_gen = generated.float() # 1 x 3 x 224 x 400\n",
        "      aug_gen = create_crops(aug_gen, num_crops=1)\n",
        "      aug_gen_norm = norm_data(aug_gen[0])\n",
        "      show_from_tensor(aug_gen_norm)\n",
        "\n",
        "    print(\"Generation\")\n",
        "    latest_gen=norm_data(generated.cpu()) # 1 x 3 x 224 x 400\n",
        "    show_from_tensor(latest_gen[0])\n",
        "\n",
        "  return (latest_gen[0])"
      ],
      "metadata": {
        "id": "HfgdJhm9z7xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimization process\n",
        "\n",
        "def optimize_result(Params, prompt):\n",
        "  alpha=1 ## the importance of the include encodings\n",
        "  beta=.5 ## the importance of the exclude encodings\n",
        "\n",
        "  ## image encoding\n",
        "  out = generator(Params())\n",
        "  out = norm_data(out)\n",
        "  out = create_crops(out)\n",
        "  out = normalize(out) # 30 x 3 x 224 x 224\n",
        "  image_enc=clipmodel.encode_image(out) ## 30 x 512\n",
        "\n",
        "  ## text encoding  w1 and w2\n",
        "  final_enc = w1*prompt + w1*extras_enc # prompt and extras_enc : 1 x 512\n",
        "  final_text_include_enc = final_enc / final_enc.norm(dim=-1, keepdim=True) # 1 x 512\n",
        "  final_text_exclude_enc = exclude_enc\n",
        "\n",
        "  ## calculate the loss\n",
        "  main_loss = torch.cosine_similarity(final_text_include_enc, image_enc, -1) # 30\n",
        "  penalize_loss = torch.cosine_similarity(final_text_exclude_enc, image_enc, -1) # 30\n",
        "\n",
        "  final_loss = -alpha*main_loss + beta*penalize_loss\n",
        "\n",
        "  return final_loss\n",
        "\n",
        "def optimize(Params, optimizer, prompt):\n",
        "  loss = optimize_result(Params, prompt).mean()\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "yKUW_gY60CrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### training loop\n",
        "\n",
        "def training_loop(Params, optimizer, show_crop=False):\n",
        "  res_img=[]\n",
        "  res_z=[]\n",
        "\n",
        "  for prompt in include_enc:\n",
        "    iteration=0\n",
        "    Params, optimizer = init_params() # 1 x 256 x 14 x 25 (225/16, 400/16)\n",
        "\n",
        "    for it in range(total_iter):\n",
        "      loss = optimize(Params, optimizer, prompt)\n",
        "\n",
        "      if iteration>=80 and iteration%show_step == 0:\n",
        "        new_img = showme(Params, show_crop)\n",
        "        res_img.append(new_img)\n",
        "        res_z.append(Params()) # 1 x 256 x 14 x 25\n",
        "        print(\"loss:\", loss.item(), \"\\niteration:\",iteration)\n",
        "\n",
        "      iteration+=1\n",
        "    torch.cuda.empty_cache()\n",
        "  return res_img, res_z"
      ],
      "metadata": {
        "id": "ZrUXHOd-0G2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "include=['A BLUE TREE IN THE FOREST']\n",
        "exclude='watermark'\n",
        "extras = \"\"\n",
        "w1=1\n",
        "w2=1\n",
        "noise_factor= .22\n",
        "total_iter=110\n",
        "show_step=10 # set this to see the result every 10 interations beyond iteration 80\n",
        "include_enc, exclude_enc, extras_enc = createEncodings(include, exclude, extras)\n",
        "res_img, res_z=training_loop(Params, optimizer, show_crop=True)\n"
      ],
      "metadata": {
        "id": "5ZWTnx6G0LI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    torch.cuda.empty_cache()\n",
        "    include=['A BLUE TREE IN THE FOREST','KIDS PLAYING IN MOON','FLOWERS DANCING']\n",
        "    exclude='watermark'\n",
        "    extras = \"\"\n",
        "    w1=1\n",
        "    w2=1\n",
        "    noise_factor= .22\n",
        "    total_iter=110\n",
        "    show_step=10 # set this to see the result every 10 interations beyond iteration 80\n",
        "    include_enc, exclude_enc, extras_enc = createEncodings(include, exclude, extras)\n",
        "    res_img, res_z=training_loop(Params, optimizer, show_crop=True)\n"
      ],
      "metadata": {
        "id": "31alIBtH0OtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate(res_z_list, duration_list):\n",
        "  gen_img_list=[]\n",
        "  fps = 25\n",
        "\n",
        "  for idx, (z, duration) in enumerate(zip(res_z_list, duration_list)):\n",
        "    num_steps = int(duration*fps)\n",
        "    z1=z\n",
        "    z2=res_z_list[(idx+1)%len(res_z_list)] # 1 x 256 x 14 x 25 (225/16, 400/16)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "      alpha = math.sin(1.5*step/num_steps)**6\n",
        "      z_new = alpha * z2 + (1-alpha) * z1\n",
        "\n",
        "      new_gen=norm_data(generator(z_new).cpu())[0] ## 3 x 224 x 400\n",
        "      new_img=T.ToPILImage(mode='RGB')(new_gen)\n",
        "      gen_img_list.append(new_img)\n",
        "\n",
        "  return gen_img_list\n",
        "\n",
        "durations=[5,5,5,5,5,5]\n",
        "interp_result_img_list = interpolate(res_z, durations)"
      ],
      "metadata": {
        "id": "vtSwnOlW3mLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create a video\n",
        "out_video_path=f\"../video.mp4\"\n",
        "writer = imageio.get_writer(out_video_path, fps=25)\n",
        "for pil_img in interp_result_img_list:\n",
        "  img = np.array(pil_img, dtype=np.uint8)\n",
        "  writer.append_data(img)\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "gLKinxuN3Wue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open('../video.mp4','rb').read()\n",
        "data=\"data:video/mp4;base64,\"+b64encode(mp4).decode()\n",
        "HTML(\"\"\"<video width=800 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data)"
      ],
      "metadata": {
        "id": "6PCutYbY37yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gJw5rv0O4CI8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}